{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Execute this block if your running this notebook in Colab"
      ],
      "metadata": {
        "id": "U8OnjCzwJn1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1iOQziZqU0v7eGaHnCZmdKEOHJP5idc_y\n",
        "!mkdir /root/.kaggle/\n",
        "!mv ./kaggle.json /root/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!pip install -q kaggle\n",
        "!kaggle competitions download -c tabular-playground-series-aug-2022\n",
        "!unzip tabular-playground-series-aug-2022.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIYuVy1akYA_",
        "outputId": "a7a7f7db-2873-43e7-fb39-050993a660ca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1iOQziZqU0v7eGaHnCZmdKEOHJP5idc_y\n",
            "To: /content/kaggle.json\n",
            "100% 64.0/64.0 [00:00<00:00, 95.6kB/s]\n",
            "Downloading tabular-playground-series-aug-2022.zip to /content\n",
            "  0% 0.00/2.27M [00:00<?, ?B/s]\n",
            "100% 2.27M/2.27M [00:00<00:00, 57.5MB/s]\n",
            "Archive:  tabular-playground-series-aug-2022.zip\n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start from here if your running this notebook in other environments"
      ],
      "metadata": {
        "id": "p7Y93ybyJwYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install feature_engine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DwnhD0mOiKs",
        "outputId": "a3811228-8760-4bb8-8068-5a465ff4595f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting feature_engine\n",
            "  Downloading feature_engine-1.5.2-py2.py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.0/290.0 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: statsmodels>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from feature_engine) (0.12.2)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.8/dist-packages (from feature_engine) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from feature_engine) (1.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.3 in /usr/local/lib/python3.8/dist-packages (from feature_engine) (1.3.5)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from feature_engine) (1.7.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.3->feature_engine) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.3->feature_engine) (2022.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->feature_engine) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->feature_engine) (1.2.0)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.8/dist-packages (from statsmodels>=0.11.1->feature_engine) (0.5.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from patsy>=0.5->statsmodels>=0.11.1->feature_engine) (1.15.0)\n",
            "Installing collected packages: feature_engine\n",
            "Successfully installed feature_engine-1.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, BatchNormalization, Dense, Dropout, concatenate\n",
        "from tensorflow.nn import sigmoid\n",
        "from tensorflow.keras.activations import swish\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from feature_engine.encoding import WoEEncoder\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Hz165QMVKCV-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading datasets"
      ],
      "metadata": {
        "id": "RYR3mLv_KS3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")"
      ],
      "metadata": {
        "id": "kqOFHCUiKSX_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data pre-processing"
      ],
      "metadata": {
        "id": "V8KDejZoKf7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(df_train, df_test):\n",
        "    data = pd.concat([df_train, df_test])\n",
        "    \n",
        "    data['m3_missing'] = data['measurement_3'].isnull().astype(np.int8)\n",
        "    data['m5_missing'] = data['measurement_5'].isnull().astype(np.int8)\n",
        "    data['area'] = data['attribute_2'] * data['attribute_3']\n",
        "\n",
        "    feature = [f for f in df_test.columns if f.startswith('measurement') or f=='loading']\n",
        "\n",
        "    \n",
        "    full_fill_dict = dict()\n",
        "    full_fill_dict['measurement_17'] = {\n",
        "        'A': ['measurement_5','measurement_6','measurement_8'],\n",
        "        'B': ['measurement_4','measurement_5','measurement_7'],\n",
        "        'C': ['measurement_5','measurement_7','measurement_8','measurement_9'],\n",
        "        'D': ['measurement_5','measurement_6','measurement_7','measurement_8'],\n",
        "        'E': ['measurement_4','measurement_5','measurement_6','measurement_8'],\n",
        "        'F': ['measurement_4','measurement_5','measurement_6','measurement_7'],\n",
        "        'G': ['measurement_4','measurement_6','measurement_8','measurement_9'],\n",
        "        'H': ['measurement_4','measurement_5','measurement_7','measurement_8','measurement_9'],\n",
        "        'I': ['measurement_3','measurement_7','measurement_8']\n",
        "    }\n",
        "\n",
        "    \n",
        "    col = [col for col in df_test.columns if 'measurement' not in col]+ ['loading','m3_missing','m5_missing']\n",
        "    a = []\n",
        "    b =[]\n",
        "    for x in range(3,17):\n",
        "        corr = np.absolute(data.drop(col, axis=1).corr()[f'measurement_{x}']).sort_values(ascending=False)\n",
        "        a.append(np.round(np.sum(corr[1:4]),3))\n",
        "        b.append(f'measurement_{x}')\n",
        "    c = pd.DataFrame()\n",
        "    c['Selected columns'] = b\n",
        "    c['correlation total'] = a\n",
        "    c = c.sort_values(by = 'correlation total',ascending=False).reset_index(drop = True)\n",
        "\n",
        "    for i in range(10):\n",
        "        measurement_col = 'measurement_' + c.iloc[i,0][12:]\n",
        "        fill_dict = {}\n",
        "        for x in data.product_code.unique() : \n",
        "            corr = np.absolute(data[data.product_code == x].drop(col, axis=1).corr()[measurement_col]).sort_values(ascending=False)\n",
        "            measurement_col_dic = {}\n",
        "            measurement_col_dic[measurement_col] = corr[1:5].index.tolist()\n",
        "            fill_dict[x] = measurement_col_dic[measurement_col]\n",
        "        full_fill_dict[measurement_col] =fill_dict\n",
        "\n",
        "    feature = [f for f in data.columns if f.startswith('measurement') or f=='loading']\n",
        "    nullValue_cols = [col for col in df_train.columns if df_train[col].isnull().sum()!=0]\n",
        "\n",
        "    for code in data.product_code.unique():\n",
        "        total_na_filled_by_linear_model = 0\n",
        "        for measurement_col in list(full_fill_dict.keys()):\n",
        "            tmp = data[data.product_code == code]\n",
        "            column = full_fill_dict[measurement_col][code]\n",
        "            tmp_train = tmp[column+[measurement_col]].dropna(how='any')\n",
        "            tmp_test = tmp[(tmp[column].isnull().sum(axis=1)==0)&(tmp[measurement_col].isnull())]\n",
        "\n",
        "            model = HuberRegressor(epsilon=1.9)\n",
        "            model.fit(tmp_train[column], tmp_train[measurement_col])\n",
        "            data.loc[(data.product_code==code)&(data[column].isnull().sum(axis=1)==0)&(data[measurement_col].isnull()),measurement_col] = model.predict(tmp_test[column])\n",
        "            total_na_filled_by_linear_model += len(tmp_test)\n",
        "\n",
        "        # non-numeric columns:\n",
        "        NA = data.loc[data[\"product_code\"] == code,nullValue_cols ].isnull().sum().sum()\n",
        "        model1 = KNNImputer(n_neighbors=3)\n",
        "        data.loc[data.product_code==code, feature] = model1.fit_transform(data.loc[data.product_code==code, feature])\n",
        "\n",
        "    data['measurement_avg'] = data[[f'measurement_{i}' for i in range(3, 17)]].mean(axis=1)\n",
        "    data['measurement_std'] = data[[f'measurement_{i}' for i in range(3, 17)]].std(axis=1)\n",
        "    data['measurement_median'] = data[[f'measurement_{i}' for i in range(3, 17)]].median(axis=1)\n",
        "    data['measurement_max'] = data[[f'measurement_{i}' for i in range(3, 17)]].max(axis=1)\n",
        "    data['measurement_min'] = data[[f'measurement_{i}' for i in range(3, 17)]].min(axis=1)\n",
        "    data['measurement_skew'] = data[[f'measurement_{i}' for i in range(3, 17)]].skew(axis=1)\n",
        "    \n",
        "    \n",
        "    \n",
        "    df_train = data.iloc[:df_train.shape[0],:]\n",
        "    df_test = data.iloc[df_train.shape[0]:,:]\n",
        "\n",
        "    woe_encoder = WoEEncoder(variables=['attribute_0'])\n",
        "    woe_encoder.fit(df_train, df_train['failure'])\n",
        "    df_train = woe_encoder.transform(df_train)\n",
        "    df_test = woe_encoder.transform(df_test)\n",
        "\n",
        "    df_train = df_train.drop(columns=['measurement_std', 'measurement_median', 'measurement_max', 'measurement_min', 'measurement_skew'])\n",
        "    df_test = df_test.drop(columns=['measurement_std', 'measurement_median', 'measurement_max', 'measurement_min', 'measurement_skew'])\n",
        "\n",
        "    return df_train, df_test"
      ],
      "metadata": {
        "id": "8sE6ltmaKiRg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, df_test = preprocessing(train, test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9NUvMcFMzso",
        "outputId": "b529b7f2-89ba-4932-f770-ab0e8a5eda23"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the models and downloading the weigths. ([Link to the model](https://drive.google.com/file/d/10EaRD58y6gF8YA688_sPbVz06xOMk7YH/view?usp=sharing))"
      ],
      "metadata": {
        "id": "ZbpUIm05LQFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 10EaRD58y6gF8YA688_sPbVz06xOMk7YH\n",
        "!unzip models.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imqr6_kLMMJy",
        "outputId": "30161362-85ab-4001-db54-c682ea330a9a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10EaRD58y6gF8YA688_sPbVz06xOMk7YH\n",
            "To: /content/models.zip\n",
            "\r  0% 0.00/2.23M [00:00<?, ?B/s]\r100% 2.23M/2.23M [00:00<00:00, 151MB/s]\n",
            "Archive:  models.zip\n",
            "  inflating: model0.h5               \n",
            "  inflating: model1.h5               \n",
            "  inflating: model2.h5               \n",
            "  inflating: model3.h5               \n",
            "  inflating: model4.h5               \n",
            "  inflating: model5.h5               \n",
            "  inflating: model6.h5               \n",
            "  inflating: model7.h5               \n",
            "  inflating: model8.h5               \n",
            "  inflating: model9.h5               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['loading', 'attribute_0', 'measurement_17', 'measurement_0', 'measurement_1', 'measurement_2', 'area', 'm3_missing', 'm5_missing', 'measurement_avg']\n",
        "NUM_FEATURE = len(features)\n",
        "DROPOUT_RATE = 0.2\n",
        "NUM_MODEL = 10"
      ],
      "metadata": {
        "id": "rrXafrksdak4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [ref] https://www.kaggle.com/competitions/tabular-playground-series-aug-2022/discussion/349385\n",
        "def get_stacked_dense(x, cur_dim, stack_depth, activation):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "        `x`: current layer\n",
        "        `cur_dim`: the number of dimensions of the first stacked layer\n",
        "        `stack_depth`: the number of stacked layers\n",
        "        `activation`: the activation function used for the dense layers\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        `x`: the output of the stacked layers\n",
        "    \"\"\"\n",
        "    for i in range(stack_depth):\n",
        "        x = Dense(cur_dim-i, activation)(x)\n",
        "    return x\n",
        "\n",
        "def get_triple_stacked(x_0, start_dim):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "        `x_0`: current layer\n",
        "        `start_dim`: the number of dimensions of the first stacked layer\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        `x_00000, x_00001, x_00010, x_00011, x_00100, x_00101, x_00110, x_00111`: the outputs of the stacked layers\n",
        "    \"\"\"\n",
        "    x_00 = Dense(start_dim, swish)(x_0)\n",
        "    x_000 = Dense(start_dim-1, swish)(x_00)\n",
        "    x_0000 = Dense(start_dim-2, swish)(x_000)\n",
        "\n",
        "    x_00000 = get_stacked_dense(x_0000, start_dim-3, 2, swish)\n",
        "    x_00000 = BatchNormalization()(x_00000)\n",
        "\n",
        "    x_00001 = get_stacked_dense(x_0000, start_dim-4, 2, swish)\n",
        "    x_00001 = BatchNormalization()(x_00001)\n",
        "\n",
        "    x_0001 = Dense(start_dim-3, swish)(x_000)\n",
        "    x_00010 = get_stacked_dense(x_0001, start_dim-4, 2, swish)\n",
        "    x_00010 = BatchNormalization()(x_00010)\n",
        "\n",
        "    x_00011 = get_stacked_dense(x_0001, start_dim-5, 2, swish)\n",
        "    x_00011 = BatchNormalization()(x_00011)\n",
        "\n",
        "    x_001 = Dense(start_dim-2, swish)(x_00)\n",
        "    x_0010 = Dense(start_dim-3, swish)(x_001)\n",
        "    x_00100 = get_stacked_dense(x_0010, start_dim-4, 2, swish)\n",
        "    x_00100 = BatchNormalization()(x_00100)\n",
        "\n",
        "    x_00101 = get_stacked_dense(x_0010, start_dim-5, 2, swish)\n",
        "    x_00101 = BatchNormalization()(x_00101)\n",
        "\n",
        "    x_0011 = Dense(start_dim-4, swish)(x_001)\n",
        "    x_00110 = get_stacked_dense(x_0011, start_dim-5, 2, swish)\n",
        "    x_00110 = BatchNormalization()(x_00110)\n",
        "\n",
        "    x_00111 = get_stacked_dense(x_0011, start_dim-6, 2, swish)\n",
        "    x_00111 = BatchNormalization()(x_00111)\n",
        "\n",
        "    return x_00000, x_00001, x_00010, x_00011, x_00100, x_00101, x_00110, x_00111\n",
        "\n",
        "def get_model(feature_num):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "        `feature_num`: the number of the features from the training dataset\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        `model`: the assembled model\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=(feature_num,))\n",
        "    x = Dense(20, swish)(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(DROPOUT_RATE)(x)\n",
        "    x_0 = Dense(20, swish)(x)\n",
        "    x_00000, x_00001, x_00010, x_00011, x_00100, x_00101, x_00110, x_00111 = get_triple_stacked(x_0, 19)\n",
        "    x_01000, x_01001, x_01010, x_01011, x_01100, x_01101, x_10110, x_01111 = get_triple_stacked(x_0, 18)\n",
        "    cat_layer_list = [x_00000, x_00001, x_00010, x_00011, x_00100, x_00101, x_00110, x_00111, x_01000, x_01001, x_01010, x_01011, x_01100, x_01101, x_10110, x_01111]\n",
        "    cat = concatenate(cat_layer_list)\n",
        "    dense1 = Dense(20, swish)(cat)\n",
        "    output = Dense(1, sigmoid)(dense1)\n",
        "    model = Model(inputs, output)\n",
        "    return model"
      ],
      "metadata": {
        "id": "lmz6e_Lhe3Sy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = np.zeros((df_test.shape[0], 1)) # ensembled results from 10 models\n",
        "for i in range(NUM_MODEL):\n",
        "\n",
        "    model = get_model(NUM_FEATURE)\n",
        "    model.load_weights(f\"model{i}.h5\")\n",
        "    y_pred = model.predict(df_test[features].values)\n",
        "    test_predictions += y_pred / NUM_MODEL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADVnGGrxLeee",
        "outputId": "95f059e2-9990-403a-f551-07d3c0242277"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "650/650 [==============================] - 3s 3ms/step\n",
            "650/650 [==============================] - 3s 3ms/step\n",
            "650/650 [==============================] - 3s 3ms/step\n",
            "650/650 [==============================] - 3s 3ms/step\n",
            "650/650 [==============================] - 3s 3ms/step\n",
            "650/650 [==============================] - 3s 3ms/step\n",
            "650/650 [==============================] - 3s 3ms/step\n",
            "650/650 [==============================] - 3s 3ms/step\n",
            "650/650 [==============================] - 3s 3ms/step\n",
            "650/650 [==============================] - 3s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing the result into a .csv file"
      ],
      "metadata": {
        "id": "rEfeigvRMsgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sunmission = pd.DataFrame({'id':df_test['id'], 'failure':test_predictions.reshape(-1)})\n",
        "sunmission.to_csv('submission_final.csv', index=False)"
      ],
      "metadata": {
        "id": "n37yohMiMxT5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the following cell if this submission is going to be submitted to Kaggle "
      ],
      "metadata": {
        "id": "AokfeWo6NR1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c tabular-playground-series-aug-2022 -f submission_final.csv -m \"Final submission\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MArQhj4NZ9C",
        "outputId": "f62624a4-66dc-4e10-92bf-38bad17d1f10"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 520k/520k [00:03<00:00, 158kB/s]\n",
            "Successfully submitted to Tabular Playground Series - Aug 2022"
          ]
        }
      ]
    }
  ]
}